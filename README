=== About =========================================================

This package contains various scripts and code for creating
"universal" parallel environments (PE).  These PEs are capable
of handling jobs utilizing the following, tested, MPI 
impementations:

 * OpenMPI
 * HPMPI
 * Intel MPI
 * MVAPICH/MVAPICH2
 * MPICH/MPICH2
 * LAM/MPI

Other non-MPI message-passing/memory-sharing implementations that
will be supported include

 * Linda (of Gaussian fame)

=== Description of Files ==========================================

startpe.sh   - Called by start_proc_args, sets up necessary 
               environment including machines files in $TMPDIR, 
               mpdboot symlinks, etc.  Available machine files are
             
               machines.mpich
               machines.mvapich
               machines.mpich2
               machines.mvapich2
               machines.intelmpi
               machines.hpmpi

stoppe.sh    - Cleans up the mess created by startpe.sh

pe_env_setup - Only very recent gridengine releases allow for 
               running parallel jobs from within an interactive
               qlogin session.  This script allows that
               functionality by simply doing

               qlogin -pe mpi.4 8
               ...
               user@node:$ source $SGE_ROOT/gepetools/pe_env_setup

               You can now load modules and run tightly-integrated
               MPI jobs using mpirun/mpiexec

mpdboot      - Takes the pain out of launching your MPD daemons
               across the cluster.  Call it prior to mpirun/mpiexec
               with any MPD-enabled MPI implementation.  Uses
               tight-integration.

extJobInfo   - How about some friggin' visibility into what your
               mpi processes are doing???

               Simply do:

               source $SGE_ROOT/gepetools/extJobInfo

               prior to your mpirun/mpiexec in your submit script.
               You'll be blessed with a file 
               
               ${JOB_NAME}.${JOB_ID}.extJobInfo

               which will have process info for your job's (currently, 
               master only) child processes including memory, cpu,
               and state information

=== Installation ==================================================

This package should be extracted to $SGE_ROOT/gepetools and be
available, in the same location on every compute node (unless you
disagree with this decision and want to modify the code, which is
quite simple).  If your SGE_ROOT is on an NFS mount, you should be
golden.

Step 1: Create the PEs

Use the createpe.sh script to create the defined PEs within the 
GE configuration.  Each argument specifies an available
processor-per-node configuration with the base PE, mpi being a 
simple round-robin allocation rule:

# cd $SGE_ROOT/gepetools
# ./createpe.sh 1 2 4 8 16

This will create the PEs mpi, mpi.1, mpi.2, ..., mpi.16

Step 2: Build the MPICH2 mpdboot wrapper

You can safely skip this step if you have moved past requiring 
mpds for MPICH2/MVAPICH2/IntelMPI.  If you still have to support
older versions that are not on the HYDRA process manager, this is
the best way to acheive tight-integration support.

# cd $SGE_ROOT/gepetools/mpdboot_wrapper
# ./aimk

If you are using an architecture other than lx26-amd64, you'll want
to run this same procedure on a node of that architecture and make 
sure the gepetools directory gets synced to the other nodes of that
architecture type.

Step 3: Submit a friggin' job already!

Examples below:

1. MPICH Example

#$ ... SGE directives ...
#$ -pe mpi.4 16   # 16 slots, 4 ppn
#$ ...

# Doesn't everyone use modules?
module purge
module add mpi/mpich/1.2.7

mpirun -np $NSLOTS -machinefile $TMPDIR/machines.mpich myexecutable
###

2. MPICH2/MVAPICH2 pre-hydra Example

#$ ...
#$ -pe mpi 32  # 32 slots, round-robin
#$ ...

module purge
module add mpi/mpich2/1.4

mpdboot
mpirun -np $NSLOTS myexecutable
###

3. MPICH2/MVAPICH2/IntelMPI w/ Hydra

#$ ...
#$ -pe mpi.8 64 # 64 slots, 8 ppn
#$ ...

module purge
module add mpi/intel/1.4

mpiexec -n $NSLOTS myexecutable
###

4. OpenMPI

#$ ...
#$ -pe mpi.12 8 # 128 slots, 8 ppn
#$ ...

module purge
module add mpi/openmpi/1.4.4

mpirun myexecutable
###
